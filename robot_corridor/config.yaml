simulation:
  corridor_length: 100.0
  corridor_width: 3.0
  robot_view_range: 4.0
  max_steps: 30000
  env_precision: 0.2
  warmup_steps: 1000

robot:
  xml_path: "four_wheels_robot.xml"
  action_smoothing_k: 5
  control_mode: "discrete_direction" # Options: "continuous_wheels", "discrete_direction", "continuous_vector"

rewards:
  goal: 100.0
  collision: -0.05
  straight_line_penalty: 0.1
  straight_line_dist: 40.0
  pacer_speed: 0.004
  # New velocity-based reward settings
  use_velocity_reward: true
  velocity_reward_scale: 0.1  # For absolute position: scales x_pos (0-100m range)

training:
  agent_type: "ppo"
  max_episodes: 20000
  model_path: "exp3_simpl.pth"
  load_weights_from: null
  learning_rate: 0.0001  # Reduced from 0.0003 for stability
  gamma: 0.99
  k_epochs: 3  # Reduced from 4 - less aggressive updates
  eps_clip: 0.2
  update_timestep: 4000
  gae_lambda: 0.95
  # New hyperparameters (previously hardcoded)
  entropy_coeff: 0.01  # Low: encourage policy to converge to consistent forward action
  value_loss_coeff: 0.5
  grad_clip_max_norm: 0.5
  action_std_init: 0.5
  action_std_min: 0.01
  action_std_max: 1.0
  actor_hidden_gain: 1.414
  actor_output_gain: 0.01
  reward_scale: 1.0
