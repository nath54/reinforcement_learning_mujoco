# Optimized configuration for faster training
# Key changes from main.yaml:
# - max_steps: 5000 (was 30000) - enforced episode cap
# - action_repeat: 5 (was 10) - faster episode completion
# - update_timestep: 2048 (was 4000) - more frequent policy updates
# - use_true_velocity: true - use actual velocity, not position
# - velocity_reward_scale: 1.0 (was 0.1) - scaled for velocity magnitude
# - entropy_coeff: 0.02 (was 0.01) - more exploration
# - num_envs: 8 (was 6) - more parallelism if GPU allows

simulation:
  corridor_length: 100.0
  corridor_width: 3.0
  robot_view_range: 4.0
  max_steps: 5000              # Reduced from 30000 - caps episode length
  env_precision: 0.2
  warmup_steps: 1000           # Kept at 1000 (robot needs to stabilize after spawn)
  action_repeat: 5             # Reduced from 10 - faster episode completion
  num_envs: 8                  # Increased from 6 - more parallelism
  obstacles_mode: "sinusoidal"
  obstacles_mode_param:
    obstacle_sep: 5.0
    obstacle_size_x: 0.4
    obstacle_size_y: 0.4
    obstacle_size_z: 0.5
  gravity: "0 0 -0.20"
  dt: 0.01
  solver: "Newton"
  iterations: 500
  ground_friction: "1 0.005 0.0001"

robot:
  xml_path: "xml/four_wheels_robot.xml"
  action_smoothing_k: 5
  control_mode: "discrete_direction"
  max_speed: 10.0

model:
  config_file: "agents/policy_mlp_small.yaml"

rewards:
  type: "velocity"
  goal: 100.0
  collision: -0.05
  straight_line_penalty: 0.1
  straight_line_dist: 40.0
  pacer_speed: 0.004
  use_velocity_reward: true
  use_true_velocity: true       # NEW: Use actual velocity instead of position
  velocity_reward_scale: 1.0    # Increased from 0.1 (velocity is smaller than position)
  stuck_penalty: -0.1
  stuck_x_velocity_threshold: 0.05
  backward_escape_bonus: 0.02

training:
  agent_type: "ppo"
  max_episodes: 20000
  model_path: "exp_optimized.pth"
  load_weights_from: null
  learning_rate: 0.0003
  gamma: 0.99
  k_epochs: 4                   # Increased from 3 - more update epochs
  eps_clip: 0.2
  update_timestep: 2048         # Reduced from 4000 - more frequent updates
  gae_lambda: 0.95
  entropy_coeff: 0.02           # Increased from 0.01 - more exploration
  value_loss_coeff: 0.5
  grad_clip_max_norm: 0.5
  reward_scale: 1.0
