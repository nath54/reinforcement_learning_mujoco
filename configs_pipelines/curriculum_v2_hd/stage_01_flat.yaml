# Stage 1: Flat Ground Navigation (Enhanced HD)
# Robot learns to navigate to random targets with improved vision resolution
# Vision offset: 0.2m ahead, Grid resolution: 0.1m (2x better)

simulation:
  scene_type: "flat_world"  # Use flat world arena
  corridor_length: 50.0  # Arena size X
  corridor_width: 50.0   # Arena size Y (symmetric)
  goal_radius: 3.0       # Success radius for goal
  randomize_goal: true   # Randomize goal each episode
  robot_view_range: 4.0
  max_steps: 10000
  env_precision: 0.1  # HD: 2x better resolution (was 0.2)
  vision_position_offset: 0.2  # Vision centered 0.2m ahead of robot
  vision_encoding_mode: "binary_with_robot"  # Show robot position as 0.5 in vision
  warmup_steps: 500
  action_repeat: 5
  num_envs: 8
  obstacles_mode: "none"
  gravity: "0 0 -0.20"
  dt: 0.01
  solver: "Newton"
  iterations: 500
  ground_friction: "1 0.005 0.0001"

robot:
  xml_path: "xml/four_wheels_robot.xml"
  action_smoothing_k: 5
  control_mode: "discrete_direction"
  max_speed: 10.0

model:
  type: "mlp"
  input_mode: "single_head"
  state_history_length: 4
  include_goal: true
  embedding_dim: 64
  hidden_sizes: [256, 256]

rewards:
  type: "velocity"
  goal: 100.0
  use_velocity_reward: true
  use_true_velocity: true
  velocity_reward_scale: 2.0
  forward_progress_scale: 5.0
  stuck_penalty: -0.05
  stuck_x_velocity_threshold: 0.02

training:
  agent_type: "ppo"
  learning_rate: 0.0003
  gamma: 0.99
  k_epochs: 4
  eps_clip: 0.2
  update_timestep: 2048
  gae_lambda: 0.95
  entropy_coeff: 0.02
  value_loss_coeff: 0.5
  grad_clip_max_norm: 0.5
  reward_scale: 1.0
