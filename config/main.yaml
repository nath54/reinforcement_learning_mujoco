simulation:
  corridor_length: 100.0
  corridor_width: 3.0
  robot_view_range: 4.0
  max_steps: 30000
  env_precision: 0.2
  warmup_steps: 1000
  action_repeat: 10
  num_envs: 6
  obstacles_mode: "sinusoidal"
  obstacles_mode_param:
    obstacle_sep: 5.0
    obstacle_size_x: 0.4
    obstacle_size_y: 0.4
    obstacle_size_z: 0.5
  gravity: "0 0 -0.20"     # Low gravity makes turning easier (matches original)
  dt: 0.01
  solver: "Newton"
  iterations: 500
  ground_friction: "1 0.005 0.0001"

robot:
  xml_path: "xml/four_wheels_robot.xml"
  action_smoothing_k: 5
  control_mode: "discrete_direction"  # Options: "continuous_wheels", "discrete_direction", "continuous_vector"
  max_speed: 10.0

# Pointer to specific configurations
model:
  config_file: "agents/policy_mlp_small.yaml"  # or agents/policy_transformer_large.yaml

rewards:
  config_file: "rewards/velocity_focused.yaml"

training:
  agent_type: "ppo"
  max_episodes: 20000
  model_path: "exp4_obstacles.pth"
  load_weights_from: null
  learning_rate: 0.0003  # Fixed: was 'lr', should be 'learning_rate'
  gamma: 0.99
  k_epochs: 3
  eps_clip: 0.2
  update_timestep: 4000
  gae_lambda: 0.95
  entropy_coeff: 0.01
  value_loss_coeff: 0.5
  grad_clip_max_norm: 0.5
  reward_scale: 1.0